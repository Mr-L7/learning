Task
----

> A unit of execution. We have two kinds of Task's in Spark:
- org.apache.spark.scheduler.ShuffleMapTask
- org.apache.spark.scheduler.ResultTask

> A Spark job consists of one or more stages. The very last stage in a job consists of multiple ResultTasks, while earlier stages consist of ShuffleMapTasks. A ResultTask executes the task and sends the task output back to the driver application. A ShuffleMapTask executes the task and divides the task output to multiple buckets (based on the task's partitioner).


## Task
```scala
private[spark] abstract class Task[T](val stageId: Int, var partitionId: Int) extends Serializable {
  final def run(attemptId: Long): T = {
    context = new TaskContext(stageId, partitionId, attemptId, runningLocally = false)
    if (_killed) {
      kill()
    }
    runTask(context)
  }

  def runTask(context: TaskContext): T

  def preferredLocations: Seq[TaskLocation] = Nil
  // Task context, to be initialized in run().
  @transient protected var context: TaskContext = _

  ...
}
```

从源代码出发，一个Task相对比较简单，一个Task里面通常包含了`run(attemptId: Long): T`函数，内容如下：
``` scala
  final def run(attemptId: Long): T = {
    context = new TaskContext(stageId, partitionId, attemptId, runningLocally = false)
    if (_killed) {
      kill()
    }
    runTask(context)
  }

  def runTask(context: TaskContext): T
```
就是为Task创建一个运行上下文，然后调用runTask来执行。而这里的runTask抽象函数，是通过其子类所实现的。因此，ShuffleMapTask和ResultTask的run的实际执行过程是不同的。其中TaskContext则主要是用来记录task执行完成后的回调函数的，在一个task执行完毕后，会通过`executeOnCompleteCallbacks()`按注册顺序的反向顺序进行回调：

``` scala
/**
 * :: DeveloperApi ::
 * Contextual information about a task which can be read or mutated during execution.
 */
@DeveloperApi
class TaskContext(
  val stageId: Int,
  val partitionId: Int,
  val attemptId: Long,
  val runningLocally: Boolean = false,
  @volatile var interrupted: Boolean = false,
  private[spark] val taskMetrics: TaskMetrics = TaskMetrics.empty()
) extends Serializable {

  @deprecated("use partitionId", "0.8.1")
  def splitId = partitionId

  // List of callback functions to execute when the task completes.
  @transient private val onCompleteCallbacks = new ArrayBuffer[() => Unit]

  /**
   * Add a callback function to be executed on task completion. An example use
   * is for HadoopRDD to register a callback to close the input stream.
   * @param f Callback function.
   */
  def addOnCompleteCallback(f: () => Unit) {
    onCompleteCallbacks += f
  }

  def executeOnCompleteCallbacks() {
    // Process complete callbacks in the reverse order of registration
    onCompleteCallbacks.reverse.foreach{_()}
  }
}
```


Task、ShuffleMapTask以及ResultTask三者之间的代码关系如下图：
![Task](./img/Task.jpg)

### ResultTask
对于`ResultTask`而言，它通常被RDD的Action所生成使用的，比如count等。所以ResultTask通常需要指定`func`，以便指明执行何种操作。至于ResultTask的主要工作便是使用`func`函数对相应split进行函数计算，并在计算完成后，通过`TaskContext`对注册过的回调函数进行回调。
``` scala
/**
 * A task that sends back the output to the driver application.
 *
 * See [[org.apache.spark.scheduler.Task]] for more information.
 *
 * @param stageId id of the stage this task belongs to
 * @param rdd input to func
 * @param func a function to apply on a partition of the RDD
 * @param _partitionId index of the number in the RDD
 * @param locs preferred task execution locations for locality scheduling
 * @param outputId index of the task in this job (a job can launch tasks on only a subset of the
 *                 input RDD's partitions).
 */
private[spark] class ResultTask[T, U](
    stageId: Int,
    var rdd: RDD[T],
    var func: (TaskContext, Iterator[T]) => U,
    _partitionId: Int,
    @transient locs: Seq[TaskLocation],
    var outputId: Int)
  extends Task[U](stageId, _partitionId) with Externalizable {
  ...
  override def runTask(context: TaskContext): U = {
    metrics = Some(context.taskMetrics)
    try {
      func(context, rdd.iterator(split, context))
    } finally {
      context.executeOnCompleteCallbacks()
    }
  }
  ...
}
```

### ShuffleMapTask


## Task的运行时

