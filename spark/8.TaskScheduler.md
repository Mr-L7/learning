Task Scheduler
--------------

在[Stage](./6.Stage.md)的分析中，我们了解了Task是由[`DAGScheduler`](https://github.com/apache/spark/blob/v0.9.1/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L740)创建Stage之后，通过`submitMissingTasks`函数来生成Task的。生成的Task通过`taskScheduler.submitTasks`来进行提交。所以接下来就是`TaskScheduler`如何调度Task的问题了。


# Trait 
`TaskScheduler`只是一个`trait`，它定义了一系列的接口，具体实现在目前的版本中，是由`TaskSchedulerImpl`完成的。`TaskScheduler`的具体接口包含如下：
``` scala
/**
 * Low-level task scheduler interface, currently implemented exclusively by TaskSchedulerImpl.
 * This interface allows plugging in different task schedulers. Each TaskScheduler schedulers tasks
 * for a single SparkContext. These schedulers get sets of tasks submitted to them from the
 * DAGScheduler for each stage, and are responsible for sending the tasks to the cluster, running
 * them, retrying if there are failures, and mitigating stragglers. They return events to the
 * DAGScheduler.
 */
private[spark] trait TaskScheduler {

  def rootPool: Pool

  def schedulingMode: SchedulingMode

  def start(): Unit

  // Invoked after system has successfully initialized (typically in spark context).
  // Yarn uses this to bootstrap allocation of resources based on preferred locations,
  // wait for slave registerations, etc.
  def postStartHook() { }

  // Disconnect from the cluster.
  def stop(): Unit

  // Submit a sequence of tasks to run.
  def submitTasks(taskSet: TaskSet): Unit

  // Cancel a stage.
  def cancelTasks(stageId: Int, interruptThread: Boolean)

  // Set the DAG scheduler for upcalls. This is guaranteed to be set before submitTasks is called.
  def setDAGScheduler(dagScheduler: DAGScheduler): Unit

  // Get the default level of parallelism to use in the cluster, as a hint for sizing jobs.
  def defaultParallelism(): Int
}
```
# TaskSchedulerImpl
``` scala
/**
 * Schedules tasks for multiple types of clusters by acting through a SchedulerBackend.
 * It can also work with a local setup by using a LocalBackend and setting isLocal to true.
 * It handles common logic, like determining a scheduling order across jobs, waking up to launch
 * speculative tasks, etc.
 *
 * Clients should first call initialize() and start(), then submit task sets through the
 * runTasks method.
 *
 * THREADING: SchedulerBackends and task-submitting clients can call this class from multiple
 * threads, so it needs locks in public API methods to maintain its state. In addition, some
 * SchedulerBackends synchronize on themselves when they want to send events here, and then
 * acquire a lock on us, so we need to make sure that we don't try to lock the backend while
 * we are holding a lock on ourselves.
 */
```

在[Glance with debug](3.Glance.with.debug.md)的分析中，我们分析过，每个Driver都首先要生成一个`SparkContext`环境，在这个过程中，`SparkContext`会调用TaskScheduler的`initialize()`以及`start()`方法，完成TaskScheduler的初始化过程，所以会在这个时候去调用`TaskSchedulerImpl`的这初始化以及启动方法。

另外，从上面的代码注释来看，`TaskSchedulerImpl`是通过`DAGScheduler`的[`submitMissingTasks`](https://github.com/apache/spark/blob/v0.9.1/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L789)方法调用它的[`submitTasks`](https://github.com/apache/spark/blob/v0.9.1/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L137)来接手Task的调度的。
因此，咱们可以先从`TaskSchedulerImpl`的`submitTasks`函数来分析。

## submitTasks

